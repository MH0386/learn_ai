{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8155794,"sourceType":"datasetVersion","datasetId":4364211}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mh0386/llama3-fine-tuning-as-napoleon-bonaparte?scriptVersionId=221500966\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"from dataclasses import dataclass, field\nfrom pprint import pprint\nfrom typing import Optional\nimport huggingface_hub as hf_hub\nimport torch\nimport wandb\nfrom datetime import datetime, timedelta, timezone\nfrom datasets import load_dataset\nfrom peft import (\n    LoraConfig,\n    PeftConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n)\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    AutoConfig,\n    DataCollatorForLanguageModeling,\n    Trainer,\n)\nimport os\nfrom trl import SFTTrainer\nfrom kaggle_secrets import UserSecretsClient","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-18T18:38:16.241094Z","iopub.execute_input":"2024-04-18T18:38:16.2414Z","iopub.status.idle":"2024-04-18T18:38:37.216883Z","shell.execute_reply.started":"2024-04-18T18:38:16.241373Z","shell.execute_reply":"2024-04-18T18:38:37.213587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git config --global credential.helper store\nhf_hub.login(token=UserSecretsClient().get_secret(\"hf\"), add_to_git_credential=True)\nwandb.login(key=UserSecretsClient().get_secret(\"wandb\"))","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:37.218866Z","iopub.execute_input":"2024-04-18T18:38:37.219805Z","iopub.status.idle":"2024-04-18T18:38:40.683021Z","shell.execute_reply.started":"2024-04-18T18:38:37.21977Z","shell.execute_reply":"2024-04-18T18:38:40.682032Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\nos.environ[\"WANDB_PROJECT\"] = \"Chatacter\"\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\nos.environ[\"WANDB_WATCH\"] = \"all\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"@dataclass\nclass ScriptArguments:\n    \"\"\"\n    These arguments vary depending on how many GPUs you have, what their capacity and features are, and what size model you want to train.\n    \"\"\"\n    per_device_train_batch_size: Optional[int] = field(default=8)\n#     per_device_eval_batch_size: Optional[int] = field(default=1)\n    auto_find_batch_size: Optional[bool] = field(default=True)\n    gradient_accumulation_steps: Optional[int] = field(default=4)\n    learning_rate: Optional[float] = field(default=2e-4)\n    max_grad_norm: Optional[float] = field(default=0.3)\n    weight_decay: Optional[int] = field(default=0.001)\n    lora_alpha: Optional[int] = field(default=32)\n    lora_dropout: Optional[float] = field(default=0.05)\n    lora_r: Optional[int] = field(default=16) \n    max_seq_length: Optional[int] = field(default=None) \n    model_name: Optional[str] = field(default=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n    dataset_name: Optional[str] = field(default=\"MH0386/napoleon_bonaparte\")\n    hf_username: Optional[str] = field(default=\"MH0386\")\n    fp16: Optional[bool] = field(default=True)\n    bf16: Optional[bool] = field(default=True)\n    packing: Optional[bool] = field(default=False, metadata={\"help\": \"Use packing dataset creating.\"})\n    gradient_checkpointing: Optional[bool] = field(default=True, metadata={\"help\": \"Enables gradient checkpointing.\"})\n    use_flash_attention_2: Optional[bool] = field(default=True, metadata={\"help\": \"Enables Flash Attention 2.\"})\n    optim: Optional[str] = field(default=\"paged_adamw_8bit\", metadata={\"help\": \"The optimizer to use.\"})\n    lr_scheduler_type: str = field(\n        default=\"linear\", #constant\n        metadata={\"help\": \"Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis\"},\n    )\n    max_steps: int = field(default=1000, metadata={\"help\": \"How many optimizer update steps to take\"})\n    warmup_ratio: float = field(default=0.05, metadata={\"help\": \"Fraction of steps to do a warmup for\"})\n    save_steps: int = field(default=100, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n    logging_steps: int = field(default=1, metadata={\"help\": \"Log every X updates steps.\"})\n    output_dir: str = field(\n        default=\"MH0386/llama-3-napoleon-bonaparte\",\n        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n    )","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:40.684521Z","iopub.execute_input":"2024-04-18T18:38:40.685115Z","iopub.status.idle":"2024-04-18T18:38:40.699918Z","shell.execute_reply.started":"2024-04-18T18:38:40.685086Z","shell.execute_reply":"2024-04-18T18:38:40.698787Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"parser = HfArgumentParser(ScriptArguments)\nscript_args = parser.parse_args_into_dataclasses(return_remaining_strings=True)[0]","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:40.701192Z","iopub.execute_input":"2024-04-18T18:38:40.701563Z","iopub.status.idle":"2024-04-18T18:38:40.714628Z","shell.execute_reply.started":"2024-04-18T18:38:40.701513Z","shell.execute_reply":"2024-04-18T18:38:40.713816Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pprint(script_args)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:40.715695Z","iopub.execute_input":"2024-04-18T18:38:40.716039Z","iopub.status.idle":"2024-04-18T18:38:40.723916Z","shell.execute_reply.started":"2024-04-18T18:38:40.716008Z","shell.execute_reply":"2024-04-18T18:38:40.722978Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def formatting_func(example):\n    full_prompt = f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are Napoleon Bonaparte<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{example['Q']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n{example['A']}<|eot_id|>\"\n    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n    return tokenized_full_prompt","metadata":{"execution":{"iopub.status.busy":"2024-04-18T19:16:29.786876Z","iopub.execute_input":"2024-04-18T19:16:29.787647Z","iopub.status.idle":"2024-04-18T19:16:30.419813Z","shell.execute_reply.started":"2024-04-18T19:16:29.787613Z","shell.execute_reply":"2024-04-18T19:16:30.418895Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:40.734474Z","iopub.execute_input":"2024-04-18T18:38:40.73482Z","iopub.status.idle":"2024-04-18T18:38:40.746096Z","shell.execute_reply.started":"2024-04-18T18:38:40.734789Z","shell.execute_reply":"2024-04-18T18:38:40.745275Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained(\n    script_args.model_name, \n    quantization_config=quantization_config,\n    device_map=\"auto\",\n#     attn_implementation=\"sdpa\" if not script_args.use_flash_attention_2 else \"flash_attention_2\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:38:40.747298Z","iopub.execute_input":"2024-04-18T18:38:40.747649Z","iopub.status.idle":"2024-04-18T18:41:48.260721Z","shell.execute_reply.started":"2024-04-18T18:38:40.747619Z","shell.execute_reply":"2024-04-18T18:41:48.259818Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.config.use_cache = False","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:41:48.261943Z","iopub.execute_input":"2024-04-18T18:41:48.26224Z","iopub.status.idle":"2024-04-18T18:41:48.266707Z","shell.execute_reply.started":"2024-04-18T18:41:48.262215Z","shell.execute_reply":"2024-04-18T18:41:48.265688Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(script_args.model_name)\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:41:48.267996Z","iopub.execute_input":"2024-04-18T18:41:48.268313Z","iopub.status.idle":"2024-04-18T18:41:49.761198Z","shell.execute_reply.started":"2024-04-18T18:41:48.26829Z","shell.execute_reply":"2024-04-18T18:41:49.760102Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=script_args.lora_r,\n    target_modules=[\n        \"q_proj\",\n        \"k_proj\",\n        \"v_proj\",\n        \"o_proj\",\n        \"gate_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"lm_head\",\n    ],\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    lora_alpha=script_args.lora_alpha,\n    lora_dropout=script_args.lora_dropout\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:41:49.76236Z","iopub.execute_input":"2024-04-18T18:41:49.762679Z","iopub.status.idle":"2024-04-18T18:41:49.767735Z","shell.execute_reply.started":"2024-04-18T18:41:49.762652Z","shell.execute_reply":"2024-04-18T18:41:49.766814Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:41:49.76877Z","iopub.execute_input":"2024-04-18T18:41:49.769065Z","iopub.status.idle":"2024-04-18T18:41:50.638561Z","shell.execute_reply.started":"2024-04-18T18:41:49.769036Z","shell.execute_reply":"2024-04-18T18:41:50.637576Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.add_adapter(peft_config=lora_config, adapter_name=\"adapter_1\")\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:41:50.639891Z","iopub.execute_input":"2024-04-18T18:41:50.640278Z","iopub.status.idle":"2024-04-18T18:41:51.406384Z","shell.execute_reply.started":"2024-04-18T18:41:50.640244Z","shell.execute_reply":"2024-04-18T18:41:51.40543Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train_dataset = load_dataset(script_args.dataset_name)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:43.012699Z","iopub.execute_input":"2024-04-18T18:42:43.013649Z","iopub.status.idle":"2024-04-18T18:42:44.070836Z","shell.execute_reply.started":"2024-04-18T18:42:43.013604Z","shell.execute_reply":"2024-04-18T18:42:44.069943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:44.072381Z","iopub.execute_input":"2024-04-18T18:42:44.07269Z","iopub.status.idle":"2024-04-18T18:42:44.078619Z","shell.execute_reply.started":"2024-04-18T18:42:44.072664Z","shell.execute_reply":"2024-04-18T18:42:44.077619Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset_maped = train_dataset[\"train\"].shuffle().map(formatting_func)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:44.079868Z","iopub.execute_input":"2024-04-18T18:42:44.080204Z","iopub.status.idle":"2024-04-18T18:42:47.838048Z","shell.execute_reply.started":"2024-04-18T18:42:44.080173Z","shell.execute_reply":"2024-04-18T18:42:47.837118Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset_maped","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.840945Z","iopub.execute_input":"2024-04-18T18:42:47.841585Z","iopub.status.idle":"2024-04-18T18:42:47.848104Z","shell.execute_reply.started":"2024-04-18T18:42:47.841533Z","shell.execute_reply":"2024-04-18T18:42:47.846884Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.device_count() > 1: # If more than 1 GPU\n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.849404Z","iopub.execute_input":"2024-04-18T18:42:47.849856Z","iopub.status.idle":"2024-04-18T18:42:47.868012Z","shell.execute_reply.started":"2024-04-18T18:42:47.849822Z","shell.execute_reply":"2024-04-18T18:42:47.867092Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.869158Z","iopub.execute_input":"2024-04-18T18:42:47.869501Z","iopub.status.idle":"2024-04-18T18:42:47.877878Z","shell.execute_reply.started":"2024-04-18T18:42:47.869467Z","shell.execute_reply":"2024-04-18T18:42:47.877012Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.87897Z","iopub.execute_input":"2024-04-18T18:42:47.879246Z","iopub.status.idle":"2024-04-18T18:42:47.88675Z","shell.execute_reply.started":"2024-04-18T18:42:47.879223Z","shell.execute_reply":"2024-04-18T18:42:47.885597Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=script_args.output_dir,\n    per_device_train_batch_size=script_args.per_device_train_batch_size,\n    gradient_accumulation_steps=script_args.gradient_accumulation_steps,\n    optim=script_args.optim,\n    save_steps=script_args.save_steps,\n    logging_steps=script_args.logging_steps,\n    learning_rate=script_args.learning_rate,\n    max_grad_norm=script_args.max_grad_norm,\n    max_steps=script_args.max_steps,\n    warmup_ratio=script_args.warmup_ratio,\n    lr_scheduler_type=script_args.lr_scheduler_type,\n    gradient_checkpointing=script_args.gradient_checkpointing,\n    fp16=script_args.fp16,\n#     bf16=script_args.bf16,\n    num_train_epochs=1,\n#     evaluation_strategy=\"steps\",\n    report_to=\"wandb\",\n    run_name=f\"llama-3-napoleon-{datetime.now(timezone(timedelta(hours=2))).strftime('%Y-%m-%d-%H-%M')}\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.887996Z","iopub.execute_input":"2024-04-18T18:42:47.888327Z","iopub.status.idle":"2024-04-18T18:42:47.896738Z","shell.execute_reply.started":"2024-04-18T18:42:47.888296Z","shell.execute_reply":"2024-04-18T18:42:47.895716Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_arguments,\n    train_dataset=train_dataset_maped,\n    tokenizer=tokenizer,\n    data_collator=collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.897849Z","iopub.execute_input":"2024-04-18T18:42:47.898138Z","iopub.status.idle":"2024-04-18T18:42:47.920985Z","shell.execute_reply.started":"2024-04-18T18:42:47.898115Z","shell.execute_reply":"2024-04-18T18:42:47.920091Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_result = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:42:47.924011Z","iopub.execute_input":"2024-04-18T18:42:47.924598Z","iopub.status.idle":"2024-04-18T18:44:12.602967Z","shell.execute_reply.started":"2024-04-18T18:42:47.924565Z","shell.execute_reply":"2024-04-18T18:44:12.601479Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:44:12.604118Z","iopub.status.idle":"2024-04-18T18:44:12.604511Z","shell.execute_reply.started":"2024-04-18T18:44:12.604317Z","shell.execute_reply":"2024-04-18T18:44:12.604333Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.model.save_pretrained(script_args.output_dir)","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:44:12.605865Z","iopub.status.idle":"2024-04-18T18:44:12.606315Z","shell.execute_reply.started":"2024-04-18T18:44:12.606094Z","shell.execute_reply":"2024-04-18T18:44:12.606113Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-04-18T18:44:12.60776Z","iopub.status.idle":"2024-04-18T18:44:12.608193Z","shell.execute_reply.started":"2024-04-18T18:44:12.60797Z","shell.execute_reply":"2024-04-18T18:44:12.607988Z"},"trusted":true},"outputs":[],"execution_count":null}]}